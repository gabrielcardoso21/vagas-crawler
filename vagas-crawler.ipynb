{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a markdown cell"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}


# Vamos começar importando as bibliotecas necessárias
import scrapy
from scrapy.crawler import CrawlerProcess
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
import json

# Define a classe do Spider
class LinkedInJobsSpider(scrapy.Spider):
    name = "linkedin_jobs"
    start_urls = [
        'https://www.linkedin.com/jobs/search/?keywords=machine%20learning%20engineer'
    ]

    def parse(self, response):
        for job in response.css('div.job-card-container__link'):
            yield {
                'title': job.css('h3::text').get(),
                'company': job.css('h4::text').get(),
                'location': job.css('span.job-card-container__metadata-item::text').get(),
                'description': job.css('p.job-card-container__description-snippet::text').get(),
            }

        next_page = response.css('a[aria-label="Next"]::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)

# Inicia o crawler e salva os dados em um arquivo JSON
process = CrawlerProcess(settings={
    'FEEDS': {
        'vagas.json': {
            'format': 'json',
            'encoding': 'utf8',
            'store_empty': False,
            'fields': None,
            'indent': 4,
        },
    },
})

process.crawl(LinkedInJobsSpider)
process.start()

# Carregar os dados coletados
with open('vagas.json', 'r') as file:
    dados = json.load(file)

df = pd.DataFrame(dados)

# Preprocessamento dos dados
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['description'].dropna())

# Aplicar K-Means para identificar requisitos comuns
kmeans = KMeans(n_clusters=5, random_state=0).fit(X)
df['cluster'] = kmeans.labels_

# Exibir os requisitos mais comuns por cluster
print("Requisitos mais comuns por cluster:")
for cluster in range(5):
    print(f"\nCluster {cluster}:")
    palavras = vectorizer.inverse_transform(kmeans.cluster_centers_[cluster])
    print(palavras)
